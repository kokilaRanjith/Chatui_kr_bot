
### ğŸ§  Ollama + Streamlit Chatbot (Dockerized)

A fully local, lightweight chatbot interface powered by [Ollama](https://ollama.com) and built using [Streamlit](https://streamlit.io). This project is containerized with Docker for seamless deployment.


## ğŸš€ Features

- ğŸ§  Uses locally running LLMs via Ollama (e.g., Mistral, LLaMA2, etc.)
- âš¡ Fast, interactive UI with Streamlit
- ğŸ” Private and offline-friendly
- ğŸ³ Containerized using Docker for portability


## ğŸ› ï¸ How to Run

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/ollama-streamlit-chatbot.git
```

### 2. Run Ollama server

```bash
Ollama list
Ollama run [model name]
```

### 3. Run Streamlit

```bash
streamlit run simple.py
```

> âœ… **Note**: Make sure you have [Ollama](https://ollama.com/download) installed and running on your machine before starting the container.

---

## ğŸ“„ Files Overview

| File             | Description                                  |
|------------------|----------------------------------------------|
| `simple.py`      | Main Streamlit app for chatbot (basic version) |
| `intermidate.py` | Alternative or extended version of chatbot app |
| `requirements.txt` | Python dependencies for the app              |
| `Dockerfile`     | Instructions to build the Docker image       |

---

## ğŸ§ª Using Ollama Models

Before running the app, pull your desired model using Ollama CLI:

```bash
ollama pull mistral
```

Then, in `simple.py` or `intermidate.py`, update the model name if needed:

```python
response = ollama.chat(model="mistral", messages=chat_history)
```

---

## ğŸ“ Access the App

Once the container is running, open your browser and navigate to:

```
http://localhost:8501
```

Start chatting with your private, local LLM!

---

## ğŸ™Œ Credits

- Built with [Streamlit](https://streamlit.io)
- Powered by [Ollama](https://ollama.com)
- Dockerized for easy sharing and deployment
```
